---
title: "DSUR Chap. 01"
author: "Peter Baumgartner"
date: "`r Sys.Date()`"
output: 
    learnr::tutorial:
        progressive: true
        allow_skip: true
        theme: cerulean
        css: css/mystyle.css
runtime: shiny_prerendered
---


```{r setup, include=FALSE}
library(learnr)
library(checkr)
library(ggplot2)
library(tidyverse)
library(modeest)
tutorial_options(exercise.checker = checkr::check_for_learnr)
knitr::opts_chunk$set(
        message = T,
        error = T,
        warning = T,
        comment = '##',
        highlight = T,
        prompt = F,
        echo = F
        )

# Global Variables --------------------------------------------------------------
source('www/check_test.R')
theme_update(plot.title = element_text(hjust = 0.5, face = "bold"))
nff <- sort(c(108, 103, 252, 121, 93, 57, 40, 53, 22, 116, 98))
```

# DSUR: Chapter 01

## Overview of Chapter 01

### Basic notions

This first chapter (pp 1--31) covers some basic statistical concepts in a general way e.g. without using R or other computational software. The examples provided uses small data sets and are easy to calculate by hand. But nevertheless I have tried to find an equivalent in R. This was also a kind of exercise for me to get familiar with basic statistic concepts in R.

The first book chapter could be divided into two part: It beginns with an introduction to the research process: How to start with initial observations, how to generate theories and hypothesis to test the research assumptions?

In this process are many basic notions explained:

+ Independent and dependent variables
+ Levels of measurement
+ Measurement error
+ Validity and reliability
+ Correlation and experimental research methods
+ Randomization

### Basic information for analysing data

The second part of the book chapter provides basic information for analysing data:

+ Frequency distribution
+ Measures of the central tendency
    - The mode
    - The median
    - The mean
+ Dispersion measures
    - The range
    - The quartiles
+ Going beyond data with a frequency distribution
    - Probability distribution
    - Z-scores
+ Fitting statistical models to data
    - Null hypothesis
    - Alternative (experimental) hypthesis
    
### Summary

Recapitulate in your mind the following questions and compare it with the text in the book chapter:

+ How do I do research?
+ What qualities should a scientific theory have?
+ What is the difference between reliability and validity?
+ What is the difference between experimental and correlation research?
+ Why is randomization important?
+ What is a frequency distribution and when is it normal?
+ What are the mode, median and mean?
+ What is the normal distribution?

## Quizzes for Chapter 01 {#quiz01}

### Quiz at the SAGE site

To check if you are conversant with these notions you could take the [multiple choice test for this chapter](https://studysites.uk.sagepub.com/dsur/study/mcq/Chapter%201/quiz.htm) taken from the student resources of the companion site. 

Additionally I have prepared another Quiz. The following 13 questions of this quiz are taken from the instructor resources at the [SAGE website](https://studysites.uk.sagepub.com/dsur/main.htm).

### Another quiz (13 Questions)

To be used in conjunction with Field, A. P., Miles, J., and Field, Z. C. (2012). [Discovering statistics using R](https://studysites.uk.sagepub.com/dsur/main.htm). London: Sage

```{r quiz-chapter-01}

quiz(caption = "Quiz Chapter 01",
  question("What is a confounding variable?",
    answer("A variable that is manipulated by the experimenter."),
    answer("A variable that affects the outcome being measured as well as or instead of the independent variable.", correct = TRUE),
    answer("A variable that has not been measured."),
    answer("A variable that is made up only of categories."),
    random_answer_order = TRUE
  ),
question("'Children can learn a second language faster before the age of 7'. Is this statement:",
    answer("A null hypothesis"),
    answer("A non‐scientific statement."),
    answer("A two‐tailed hypothesis."),
    answer("A one‐tailed hypothesis.", correct = TRUE),
    random_answer_order = TRUE
  ),

question("If a psychological test is valid, what does this mean?",
    answer("The test will give consistent results."),
    answer("The test measures what it claims to measure.", correct = TRUE),
    answer("The test has internal consistency."),
    answer("The test measures a psychologically useful variable."),
    random_answer_order = TRUE
),

question("If my null hypothesis is ‘Dutch people do not differ from English people in height’, what is my alternative hypothesis?",
    answer("Dutch people are taller than English people."),
    answer("English people are taller than Dutch people."),
    answer("Dutch people differ in height from English people."),
    answer("All of the above are plausible alternative hypotheses.", correct = TRUE),
    random_answer_order = TRUE
),

question("When questionnaire scores predict, or correspond with, external measures of the same construct that the questionnaire measures it is said to have:",
    answer("Ecological validity."),
    answer("Factorial validity."),
    answer("Content validity."),
    answer("Criterion validity.", correct = TRUE),
    random_answer_order = TRUE
),

question("A variable manipulated by a researcher is known as:",
    answer("A dependent variable."),
    answer("A confounding variable."),
    answer("A discrete variable."),
    answer("An independent variable.", correct = TRUE),
    random_answer_order = TRUE
),

question("A predictor variable is another name for:",
    answer("A dependent variable."),
    answer("A confounding variable."),
    answer("A discrete variable."),
    answer("An independent variable.", correct = TRUE),
    random_answer_order = TRUE
),

question("What kind of variable is IQ, measured by a standard IQ test?",
    answer("Categorical."),
    answer("Discrete."),
    answer("Nominal."),
    answer("Continuous.", correct = TRUE),
    random_answer_order = TRUE
),

question("A frequency distribution in which high scores are most frequent (i.e. bars on the graph are highest on the right‐hand side) is said to be:",
    answer("Positively skewed."),
    answer("Leptokurtic."),
    answer("Platykurtic."),
    answer("Negatively skewed.", correct = TRUE),
    random_answer_order = TRUE
),

question("A frequency distribution in which there are too few scores at the extremes of the distribution is said to be:",
    answer("Positively skewed."),
    answer("Leptokurtic.", correct = TRUE),
    answer("Platykurtic."),
    answer("Negatively skewed."),
    random_answer_order = TRUE
),

question("Which of the following is designed to compensate for practice effects?",
    answer("A repeated measures design."),
    answer("Randomization of participants."),
    answer("Counterbalancing.", correct = TRUE),
    answer("A control condition."),
    random_answer_order = TRUE
),

question("Variation due to variables that have not been measured is known as:",
    answer("Unsystematic variance.", correct = TRUE),
    answer("Homogeneous variance."),
    answer("Systematic variance."),
    answer("Model variance."),
    random_answer_order = TRUE
),

question("If the scores on a test have a mean of 26 and a standard deviation of 4, what is the z‐score for a score of 18?",
    answer("-2", correct = TRUE),
    answer("11"),
    answer("2"),
    answer("-1.41"),
    random_answer_order = TRUE
    )
)
```



## Normal distributions

erewrewrwe

## Skewed distributions

dsfsdafdsaf

### Introductory video

<p><img src="https://www.youtube.com/watch?v=24YCewQ8s3M" /></p>

### Exploration of skewed distributions

```{r create-distributions, context="data"}
n = 10000
shape1 = 5
shape2 = 2
set.seed(1234) # tested also with 1111
d.normal <- rbeta(n, shape1, shape1)
d.right <-  rbeta(n, shape2, shape1)
d.left <-   rbeta(n, shape1, shape2)
``` 


```{r my-dist-ui1, context="render"}
    fluidRow(
        column(3,
            radioButtons("type", "Choose a distribution:",
                           c("Normal" = "norm",
                             "Right-skewed" = "right",
                             "Left-skewed" = "left")
                        ),
            checkboxInput("newDist", "Create a new distribution?", FALSE)
            ),
        column(9, htmlOutput("distText"))
    ) # end of fluidRow
    fluidRow(
        column(8,
              plotOutput("distPlot")
              )
)





```



```{r my-dist-server1,  context="server"}
library(modeest)

explainNormal <- "The <b>normal distribution</b> is the most important and most widely used distribution in statistics. It is sometimes called the <i>bell curve</i> or <i>Gaussian curve</i>. Examples are: Height of people, blood pressure, points on a test.
<br />
Some features you can see here: 
<ul>
<li>The curve is symmetric at the center.</li>
<li>The curve is denser in the center and less dense in the tails.</li>
<li>The mean, mode and median are all equal.</li>
</ul>"

    
explainRight  <- "This is a <b>right-skewed distribution</b>, also called a positevely skewed distribution. This naming schema could be confusing (it was at least for me!) as you have to draw the name from the tail (and not from the peak) of the distribution. Here the longer tail is on the right resp. positive side, therefore it is a right-skewed or positevely skewed distribution.</br>

Another important issue to remember this naming schema: On this type of distribution is the mean always situated more on the right resp. positive side of mode and median."

explainLeft   <- "This is the <b>left-skewed distribution</b>, also called a negatively skewed distribution. Here the longer tail is on the left resp. negative side of the peak. Therefore it is the a left-skewed or negatively skewed distribution.</br>

Another important issue to remember this naming schema: On left-skewed distributions is the mean always situated more on the left resp. negative side of mode and median."

n = 10000
mean = 0.5
sd = 0.15
method = "beta"
shape1 = 5
shape2 = 2
binwidth = .025
b  <- seq(0, 1, by = binwidth)
set.seed(1234)

 output$distPlot <- renderPlot({
    switch(input$type,
        norm    = {modusIntercept <- mlv(method, shape1, shape1)[[1]]
                    ifelse (input$newDist, 
                            histo <- as_tibble(rbeta(n,shape1, shape1)),
                            histo <- as_tibble(d.normal))
                    mean = 0.5
                    sd = 0.15
                    caption <- "Normal distribution"
                    output$distText <- renderText(explainNormal)},
        right   = {modusIntercept <- mlv(method, shape2, shape1)[[1]]
                    ifelse (input$newDist,
                            histo <- as_tibble(rbeta(n,shape2, shape1)),
                            histo <- as_tibble(d.right))
                   mean = 0.20
                   sd = 0.16
                   caption <- "Positively or right-skewed distribution"
                   output$distText <- renderText(explainRight)},
        left    = {modusIntercept <- mlv(method, shape1, shape2)[[1]]
                    ifelse (input$newDist, 
                            histo <- as_tibble(rbeta(n,shape1, shape2)),
                            histo <- as_tibble(d.left))
                   mean = 0.80
                   sd = 0.16
                   caption <- "Negatively or left-skewed distribution"
                   output$distText <- renderText(explainLeft)}
      )

ggplot(histo, aes(x = value, mean = mean, sd = sd, binwidth = binwidth, n = n)) +
    geom_histogram(aes(y = ..count..),
           alpha = 0.25,
           breaks = b,
           binwidth = binwidth,  
           colour = "black", 
           fill = "white") +
    geom_line(aes(y = ..density.. * n * binwidth, 
                  colour = " Empirical distribution"),
           size = 1, stat = 'density') +
    stat_function(fun = function(x) 
           {dnorm(x, mean = mean, sd = sd) * n * binwidth}, 
           aes(colour = " Normal distribution"), size = 1) +
    labs(x = "Score", y = "Frequency", title = caption) +
    geom_vline(
        aes(colour = "Mode (empirical)",
        xintercept = modusIntercept),
        linetype = "longdash",
        # color = "black",
        size = 1.5
        ) +
    geom_vline(
        aes(colour = "Mean (empirical)",
        xintercept = mean(histo$value)),
        linetype = "longdash",
        # color = "blue",
        size = 1.5
        ) +
    geom_vline(
        aes(colour = "Median (empirical)",
        xintercept = median(histo$value)),
        linetype = "longdash",
        # color = "tomato",
        size = 1.5
        ) +
    scale_colour_manual(name = "Colors",
                        values = c("ForestGreen", "#FF00FF",
                                   "black", "blue", "tomato"))
    
})
```



## Distributions with kurtosis

### Introductory video

## Calculaton of measures

### Overview

What follows are explanations how to calculate measures in R for describing frequency distributions. We distinguish:

**Measures to describe the centre of a distribution**

+ The mode
+ The median
+ The mean

**Measures to describe the dispersion of a distribution**

+ The range
+ The quartiles
+ The interquartile range

### Preparing the data set

To demonstrate the calculation we will use the same data set as in Chapter 01 of DSUR. 11 facebook user have the following number of friends: `108, 103, 252, 121, 93, 57, 40, 53, 22, 116, 98`. In R we will need the following steps:

1. **Line 1:** We create a variable `nff` (number of facebook friends) and assign all the above values to it. This happens in R with the combine-function `c()`. In parenthesis we put all the values as comma separated numbers and assign it with the `<-` operator to `nff`.
3. **Line 2:** For an easier calculation we will sort the resulted vector in `nff` and assign it to `nff` again. This happens with the `sort()` function. [A vector is a sequence of data elements of the same basic type.]
4. **Line 3:** Finally we want to inspect the result by printing the content of `nff`.


```{r create-nff-data-set, exercise=TRUE}
nff <- c(108, 103, 252, 121, 93, 57, 40, 53, 22, 116, 98)
nff <- sort(nff)
nff
```


## The Mode

When I first looked how to calculate the mode of a vector I was suprised to learn that there is no mode function in the `base` R module. Or better said, there is a `mode()` function, but it tells you the internal storage mode of the object, not the value that occurs the most in its argument.

We will therefore use the `modeest` package ("mode estimation"). The four lines of the next R chunk are explained as follows:

1. **Line 1:** The first line `library(modeest)` loads the package into memory. If you are working directly in R (e.g. without this interactive tutorial) you have probably to install the package with `install.packages("modeest")`. You will learn how to install and load packages later in another chapter of this tutorial.

2. **Line 2:** We need to have at least two numbers with the same values to get the frequencey of (at least) one number higher than the other figures. We therefore create a new temporary variable `nff_temp` and add the number 53 to our previous variable. 

3. **Line 3:** As a result we will have the number 53 twice in `nff_temp`. We check this by printing the content of `nff_temp`.

4. **Line 4:** `mfv()` is one of the many functions of the `modeest`package. It returns the **m**ost **f**requent **v**alue(s) in a given numerical vector. To calculate the mode we use this function with our vector variable `nff_temp`.

```{r compute-mode, exercise=TRUE}
library(modeest)
nff_temp <- c(nff, 53)
nff_temp
mfv(nff_temp)
```


## The Median

### Compute the median of `nff`

In contrast to mode there is a special function `median()` in the basic distribution of R. 

This time you have to write the command yourself! It is a one-liner. Use the function `median()` to compute the median for the `nff` variable. 


```{r compute-median, exercise=TRUE}

```



```{r compute-median-check}
check_compute_median(USER_CODE)
```



### Compute the median of `nff` without the outlier

The median is the middle value of the sorted vector variable `nff`. We have in `nff` 11 values: `22, 40, 53, 57, 93, 98, 103, 108, 116, 121, 252`. So the middle value is the sixth number = 98. 

In inspecting our vector variable we see that the last value `252` is much higher than all the other values. Between `121` and `252` there is an unusual big gap. The number `252` lies outside the normal sequence range and is therefore called `outlier`. Some measures (like the maen and the range) are very sensible against outliers. Others measures (like median and interquartile range) are more robust.

We can subset our original `nff` vector in R with a command in square brackets: `[1:10]` means that we only take into account the values 1-10, e.g. excluding `252`, the 11th value in our sorted vector variable. The new median is now slightly lower:

```{r compute-median-2, exercise=TRUE}
median(nff[1:10])

```

The new median is now slightly lower than `98`. In this case we have an even number of values, so there is no given middle value as median. We have to calculate the middle value by summing up the two values in the middle of the sorted sequence and then divide them by two: `(93 + 98) \ 2`

## The Mean

### Compute the mean of `nff`

The caluculation of the mean is similar as in the previous exercise the computation of the median. The only difference: The name of the function is `mean()`.

Compute the mean of `nff` yourself!

```{r compute-mean, exercise=TRUE}

```



```{r compute-mean-check}
check_compute_mean(USER_CODE)
```

### Compute the mean of `nff` without the outlier

When we compute the mean without the outlier, we will get a much bigger difference than in the computation of the median with and without outlier. Median: 98 vs. 95.5 and now we get 96.6 vs. ?. (Click the button "Run Code") 


```{r compute-mean-2, exercise=TRUE}
mean(nff[1:10])
```

Conclusion: The mean is not robust against outliers. If you have a distribution with some extreme values than the median is the besster measure to describe the centrality of the distribution.

## The Range

### Compute the range of `nff`

There is also a function range(). But this time the function does not return one but two values: The lowest and the highest value of the sequence.

```{r compute-range, exercise=TRUE}

```



```{r compute-range-check}
check_compute_range(USER_CODE)
```

### Subtracting lower value from higher value

To get the value of the range as one figure we have to subtract the smaller (first) from the higher (second) number. This is done again with the subsetting command in square brackets. Here is the complete code:

```{r compute-range-value, exercise=TRUE}
xr <- range(nff)
x <- xr[2] - xr[1]
x
```


### Compute the range of `nff` without the outlier

The range is extremly sensible against outliers: 

```{r compute-range-2, exercise=TRUE}
xr <- range(nff[1:10])
x <- xr[2] - xr[1]
x
```

### Other dispersion measures

With the exclusion of just _one_ outlier the range has dropped dramatically from 230 to 99. This is less as the half size!

One way to prevent this large sensibility against outliers is to cut values at both extreme ends. This is done with the next measures.

## Upper and Lower Quartile

The quartiles divides the sequence in 4 equal parts: 
1^st^ Part:  0% -  25%, 
2^nd^ Part: 25% -  50%
3^rd^ Part: 50% -  75%
4^th^ Part: 85% - 100%

50% is the median we have already computed with another method. 25% is called the `lower quartile` and 75% the `higher quartile`.

The function to compute these 4 parts is _not_ quartile() but `quantile()`. Quartile (divide into four parts) does not exist; is a special case of a quantile (divide into n parts), where `n` stands for the number of parts. But because the division into 4 parts is very common it is the default division and you need no further specification of the quantile() function.

So you can use the same structure of the command as in the previous examples. Try it!

```{r compute-quantile, exercise=TRUE}

```


```{r compute-quantile-check}
check_compute_quantile(USER_CODE)
```

### Upper and lower quartile again

Are you suprised about the outcome of the previous calculation? The result is not identical with the values in the book: Our result was: 

+ 22, 55, 98, 112 and 252 whereas the result in the book (p.25) is 
+ 22, 53, 98, 116 and 252

The reason for this mismatch are different computational methods. There exist 9 types of calculations. I do not want to go into the details, but with `type = 1` we will get the same results as in the book:

```{r compute-quantile-again, exercise=TRUE}
quantile(nff, type = 1)
```


## The Interquartile Range

### Compute the `IQR()` with `type = 1`

It is the convention to cut off the bottom and top 25% of the scores and calculate the range of the middle 50% of scores. This measure is known as `interquartile range`.

The name of the function is `IQR()`. The capial letters are important as R is case sensitive. 

To get the same result as in the book we have to specify again the method of caluclation we want to use (`type = 1`):


```{r compute-IQR, exercise=TRUE}
IQR(nff, type = 1)
```

### Experiment with different computational methods

You can you back and replace the number in the command with a figure from 2-9 and calculate the IQR with different methods. 

### Have a first look at help pages

The algrotihms used for this different methods are complex but explained in detail in special help pages. You can see these help pages for every function whenever you type `?<functioname>` e.g. `?IQR`. Try it! 

Replace the line of the last example with a help command e.g. `?mean` and run the code. Hopefully you are not intimitaded by the complexity of these pages. In later chapters I will explain the structure and content of these pages. For the moment: Just play a little around with the exercises!



## Self-Tests

### Self-test p.25

```{r self-test-p25}
treadmill <- c(18,16,18,24,23,22,22,23,26,29,32,34,34,36,36,43,42,49,46,46,57)

Mode(treadmill)
median(treadmill)
mean(treadmill)
quantile(treadmill, type = 6)
range(treadmill)
IQR(treadmill, type = 6)

```

### Self-Test p.27

What's the probability that someone who threw themselves of Beachy Head was 30 years or older?

* First we convert 30 into a z-score. Suppose the mean of the suicide scores was 36, and the standard deviation 13; then 30 will bekomme (30-36)/13 = `r n <- (30-36)/13; n`.
* We then look up this value in the column labelled "Bigger Portion" (i.e., the area above the value `r n`).
* I get the value of 32.28%, or put another way, there is a chance of 32.28% that a suicide victim was aged 30 or less. We can also say that there is a 67.72% chance that a suicide victim was older than 30.


